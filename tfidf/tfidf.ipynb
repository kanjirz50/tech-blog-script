{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDFを適用する\n",
    "\n",
    "## モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T15:52:32.613976Z",
     "start_time": "2018-08-30T00:52:31.713976+09:00"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.tokenfilter import POSKeepFilter, CompoundNounFilter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析器インスタンスの作成\n",
    "janomeのAnalyzerクラスを利用します。\n",
    "複合名詞処理をした後に、名詞のみをフィルタリングします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T15:52:34.733979Z",
     "start_time": "2018-08-30T00:52:32.905978+09:00"
    }
   },
   "outputs": [],
   "source": [
    "a = Analyzer(token_filters=[CompoundNounFilter(), POSKeepFilter(\"名詞\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文書集合を読み込む\n",
    "globでファイル一覧を取得し、1ファイルずつ読み込みます。\n",
    "一つのファイルは一つの文書として、docsに追加されます。\n",
    "\n",
    "一つの文書はスペース区切りの単語で構成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T15:52:36.236096Z",
     "start_time": "2018-08-30T00:52:34.736978+09:00"
    }
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for f in glob.glob(\"./docs/*.txt\"):\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fin:\n",
    "        doc = []\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            doc.append(\" \".join([tok.surface for tok in a.analyze(line)]))\n",
    "        docs.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDFベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T15:52:36.255976Z",
     "start_time": "2018-08-30T00:52:36.238980+09:00"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vector = vectorizer.fit_transform(docs)\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重要な単語上位10件を表示\n",
    "TF-IDFスコアが高い単語上位10語を表示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T15:52:36.283979Z",
     "start_time": "2018-08-30T00:52:36.258977+09:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['参加' 'cnn' '学会' 'チュートリアル' 'icdar2017' '発表' 'analysis' '議論' 'recognition'\n",
      "  '投稿']]\n",
      "[['hug' 'api' 'パラメータ' 'cli' 'コマンドラインツール' '引数' 'デコレータ' 'ラップ' 'webapi'\n",
      "  'world']]\n",
      "[['cchardet' 'beautifulsoup' 'requestsモジュール' '文字化け' 'apparent_encoding'\n",
      "  '文字コード' 'response' 'html' 'ページ' 'レスポンスヘッダ']]\n",
      "[['xonsh' 'プロセス置換' 'コマンド' 'よう' 'python' 'subprocess' 'ファイル' 'zsh' 'bash'\n",
      "  'mom']]\n",
      "[['ergodoxez' '購入' 'ため' '手数料' '開封' 'ergodoxezライフ' '猫背解消' '注文' 'hhkb' '記載']]\n",
      "[['インストール' 'sh' 'catboost' 'lightgbm' 'gpu' 'gce' 'ログインパスワード' 'engine'\n",
      "  'pyenv' 'with']]\n",
      "[['素性抽出' 'テンプレート' '素性' 'surface' '抽出' 'タグ' '対象単語' '関数' '推定' 'よう']]\n",
      "[['自然言語処理' '文書分類' '勉強会' '実装' 'ハンズオン' 'みなさま' 'テーマ' '勉強' '参加' '得意']]\n",
      "[['企業' '勉強会' '自然言語処理' 'アルゴリズム' '取り組み' 'ため' '開発' '企業特定' '企業辞書' '曖昧さ']]\n"
     ]
    }
   ],
   "source": [
    "for vec in vector:\n",
    "    index = np.argsort(vec.toarray(), axis=1)[:,::-1]\n",
    "    feature_words = feature_names[index]\n",
    "    print(feature_words[:,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 元ネタとなっている文書を表示する\n",
    "複合名詞処理＋名詞フィルタリングをした文書を先頭から200文字表示する。\n",
    "前で示した上位10件が特徴づける単語になっているか確認でき"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T15:52:36.296978Z",
     "start_time": "2018-08-30T00:52:36.285978+09:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICDAR2017 参加 最近 会社 技術ブログ うつつ 個人ブログ 投稿 久しぶり 投稿 個人ブログ 会社スポンサーブース対応 聴講 目的 京都 開催 ICDAR2017 参加 これ 言語処理系 学会 参加 画像系 ため 新鮮 参加 所感 ICDAR International Conference on Document Analysis and Recognition 略 文書 構造解析 O\n",
      "------------------------------------------------------------\n",
      "hug API CLI Sansan Advent Calendar 2017 1日目 記事 Python WebAPI コマンドラインツール とき ボトルネック がち の ルーティング 引数 管理 hug ここらへん Pythonモジュール hug WebAPI コマンドラインツール 作成 備忘録 hug WebAPI きまり Hello World! パラメータ デプロイ とき コマンドライン\n",
      "------------------------------------------------------------\n",
      "Python requestsモジュール 文字コード対策 編集 Webスクレイピング Advent Calendar 2017 4日目 記事 Python requestsモジュール Requests 人 よう 設計 Python Apache2 Licensed ベース HTTPライブラリ 公式サイト1文目 記述 HTTPライブラリ requestsモジュール 日本語HTML 対象 取得 際 文\n",
      "------------------------------------------------------------\n",
      "Xonsh Xonsh Advent Calendar 2017 13日目 記事 Xonsh 話 これ Xonsh the xonsh shell ~ こちらトム少佐 Xonsh地上管制 ~ Xonsh Python 動作 クロスプラットフォーム Unix よう シェル言語 コマンドプロンプト 言語 Python 3.4+ 上位互換 Bash IPython 基本的 シェル命令 追加 もの Lin\n",
      "------------------------------------------------------------\n",
      "ErgoDoxEZ 購入 編集 HHKB Pro 2 type-S 日本語配列 左右分離型キーボード ErgoDoxEZ 移行 目的 猫背 解消 軽減 よう きっかけ 購入 紹介 猫背解消 きっかけ 徹底的 猫背解消 ため 現在猫背対策 スタンディングデスク ディスプレイ さ 気 ラットプルダウン デッドリフト 始め 筋トレ 中 HHKB 長所 スリム キーボード 体格 こと キー入力 肩 前 左\n",
      "------------------------------------------------------------\n",
      "Google Compute Engine Python GPU環境 構築 機械学習環境 Google Compute Engine(GCE) GPUインスタンス上 Python 構築 GPU対応版 LightGBM Catboost インストール 記事 備忘録 導入プラン マシンタイプ n1-highmem-8 (vCPUx8 メモリ52GB) Ubuntu 18.04 GPU 1 x NVID\n",
      "------------------------------------------------------------\n",
      "系列ラベリング 素性抽出 系列ラベリング問題 際 素性抽出 複雑 がち テンプレート サクッ 抽出 よう整理 素性 抽出 固有表現抽出 例 以下 表 午前８時 東京駅 集合 文 形態素解析 IOB2(Inside-outside-beggining)タグ形式 固有表現 ラベル 付与 もの 午前８時 TIME属性 東京駅 LOCATION属性 こと ここ 東京 単語 例 素性抽出 素性 対象単語 前\n",
      "------------------------------------------------------------\n",
      "サポーターズ勉強会 文書分類 ハンズオン 7月31日 文書分類 自然言語処理 タイトル 講師 よう 機会 記事 題目 理由 講演 文書分類 自然言語処理 テーマ の 勉強会 講師 上 自分 こと 題目 何 専門 自然言語処理 実装 得意 ため テーマ 1度 勉強会 時間的制約 中 タスク 観点 文書分類 自然言語処理 魅力的 トピック たくさん 説明 実装 大変 新聞 雑誌 カテゴリ分け 文書分類\n",
      "------------------------------------------------------------\n",
      "会社 勉強会 7月18日 自社開催 勉強会 登壇 記事 の 気 1ヶ月 ( はず ) 勉強会 こと 感想 自然言語処理(NLP)領域 話 壮大 テーマ もと 1回目 R&D 外部向け勉強会 開催 多大 サポート 人事部 みなさま 感謝 勉強会 何 正直 発表自体 特別 アルゴリズム 高度 手法 わけ 割 地味 こと サービス 稼働 こと 開発 運用 話 テーマ Eightニュースフィード活性化 た\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc[:200])\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
